import { supabaseAdmin, retrySupabaseCall, checkSupabaseConnectivity } from '../config/database.js';
import { logger } from '../utils/logger.js';
import { kaspaApi } from './kaspaApi.js';

// System user ID for automated operations - will be initialized on first use
let SYSTEM_USER_ID = null;

// Function to ensure system user exists and get its ID
export async function getSystemUserId() {
  if (SYSTEM_USER_ID) {
    return SYSTEM_USER_ID;
  }

  try {
    // First, try to find existing system user
    const { data: existingUser, error: findError } = await retrySupabaseCall(async () => {
      return await supabaseAdmin
        .from('users')
        .select('id')
        .eq('email', 'system@kaspunk-predictor.internal')
        .eq('role', 'admin')
        .maybeSingle();
    });

    if (findError && findError.code !== 'PGRST116') {
      logger.error('Error finding system user:', findError);
      throw findError;
    }

    if (existingUser) {
      SYSTEM_USER_ID = existingUser.id;
      logger.debug('Found existing system user:', SYSTEM_USER_ID);
      return SYSTEM_USER_ID;
    }

    // System user doesn't exist, create it
    logger.info('Creating system user for automated operations...');
    
    const { data: newUser, error: createError } = await retrySupabaseCall(async () => {
      return await supabaseAdmin
        .from('users')
        .insert({
          email: 'system@kaspunk-predictor.internal',
          role: 'admin'
        })
        .select('id')
        .single();
    });

    if (createError) {
      logger.error('Error creating system user:', createError);
      throw createError;
    }

    SYSTEM_USER_ID = newUser.id;
    logger.info('Created system user with ID:', SYSTEM_USER_ID);
    return SYSTEM_USER_ID;

  } catch (error) {
    logger.error('Failed to get or create system user:', error);
    throw new Error(`Failed to initialize system user: ${error.message}`);
  }
}

// Simplified trait categories cache - no caching for one-way loader
async function getTraitCategories() {
  // Fetch fresh data every time - simpler and more reliable for one-way loading
  const { data: categories, error } = await supabaseAdmin
    .from('trait_categories')
    .select('id, name, display_order')
    .order('display_order', { ascending: true });

  if (error) {
    logger.error('Error fetching trait categories:', error);
    throw error;
  }

  // Build the categories map
  const categoriesMap = new Map();
  let maxDisplayOrder = 0;
  
  categories.forEach(cat => {
    categoriesMap.set(cat.name.toLowerCase(), {
      id: cat.id,
      display_order: cat.display_order
    });
    if (cat.display_order > maxDisplayOrder) {
      maxDisplayOrder = cat.display_order;
    }
  });

  return { categoriesMap, nextDisplayOrder: maxDisplayOrder + 1 };
}

// SIMPLE: One-way trait loader - only insert if no traits exist
export async function syncTokenTraits(ticker, tokenId, kaspaTraits) {
  if (!kaspaTraits || Object.keys(kaspaTraits).length === 0) {
    logger.debug(`No traits to sync for token ${tokenId}`);
    return;
  }

  try {
    // Ensure tokenId is a number
    const numericTokenId = parseInt(tokenId);
    if (isNaN(numericTokenId)) {
      throw new Error(`Invalid token ID: ${tokenId}`);
    }

    logger.debug(`üé® Starting one-way trait load for token ${numericTokenId}`);

    // Step 1: Check if token already has traits - if so, skip entirely
    const { data: existingTraits, error: checkError } = await supabaseAdmin
      .from('trait_data')
      .select('id')
      .eq('token_id', numericTokenId)
      .limit(1);

    if (checkError) {
      logger.error(`Error checking existing traits for token ${numericTokenId}:`, checkError);
      throw checkError;
    }

    if (existingTraits && existingTraits.length > 0) {
      logger.debug(`‚úÖ Token ${numericTokenId} already has traits, skipping`);
      return;
    }

    // Step 2: Get trait categories (no caching)
    const { categoriesMap, nextDisplayOrder } = await getTraitCategories();
    let currentDisplayOrder = nextDisplayOrder;

    // Step 3: Process each trait and prepare for insertion
    const traitsToInsert = [];
    const newCategories = [];

    for (const traitName in kaspaTraits) {
      const trait = kaspaTraits[traitName];
      if (trait && trait.value !== null) {
        const categoryName = traitName.toLowerCase();
        let categoryId = categoriesMap.get(categoryName)?.id;

        // If category doesn't exist, prepare to create it
        if (!categoryId) {
          const newCategoryId = crypto.randomUUID();
          newCategories.push({
            id: newCategoryId,
            name: categoryName,
            display_order: currentDisplayOrder++
          });
          
          // Update map immediately for this session
          categoriesMap.set(categoryName, {
            id: newCategoryId,
            display_order: currentDisplayOrder - 1
          });
          
          categoryId = newCategoryId;
        }

        // Prepare trait for insertion
        traitsToInsert.push({
          token_id: numericTokenId,
          trait_name: traitName,
          trait_value: trait.value,
          rarity: trait.rarity,
          category_id: categoryId,
          updated_at: new Date().toISOString()
        });
      }
    }

    // Step 4: Insert new categories if any
    if (newCategories.length > 0) {
      const { error: categoryError } = await supabaseAdmin
        .from('trait_categories')
        .insert(newCategories);

      if (categoryError) {
        logger.error(`Error inserting trait categories:`, categoryError);
        throw categoryError;
      } else {
        logger.debug(`‚úÖ Inserted ${newCategories.length} new trait categories`);
      }
    }

    // Step 5: Insert all traits for this token
    if (traitsToInsert.length > 0) {
      const { error: insertError } = await supabaseAdmin
        .from('trait_data')
        .insert(traitsToInsert);

      if (insertError) {
        logger.error(`Error inserting traits for token ${numericTokenId}:`, insertError);
        throw insertError;
      } else {
        logger.debug(`‚úÖ Inserted ${traitsToInsert.length} traits for token ${numericTokenId}`);
      }
    }

    logger.debug(`üé® Successfully completed one-way trait load for token ${numericTokenId}`);

  } catch (error) {
    logger.error(`Error syncing traits for token ${tokenId}:`, error);
    throw error;
  }
}

// SIMPLIFIED: Fast single token trait sync - one-way loader
export async function syncSingleTokenTraits(ticker, tokenId) {
  const startTime = Date.now();
  
  try {
    // Ensure tokenId is a number
    const numericTokenId = parseInt(tokenId);
    if (isNaN(numericTokenId)) {
      throw new Error(`Invalid token ID: ${tokenId}`);
    }

    logger.debug(`üé® Starting trait sync for ${ticker} token ${numericTokenId}`);

    // Fetch token traits using the API
    const kaspaTokenData = await kaspaApi.fetchTokenTraits(ticker, numericTokenId);

    if (!kaspaTokenData) {
      logger.debug(`üé® No token data found for ${ticker} token ${numericTokenId}`);
      return { 
        action: 'no_data', 
        tokenId: numericTokenId, 
        traitsFound: 0,
        traitsSynced: 0,
        errors: 0,
        durationMs: Date.now() - startTime
      };
    }

    // Count traits before sync
    const traitsFound = kaspaTokenData.traits ? Object.keys(kaspaTokenData.traits).length : 0;
    
    // Sync traits if we have any (one-way loader)
    if (traitsFound > 0) {
      await syncTokenTraits(ticker, numericTokenId, kaspaTokenData.traits);
    }

    // Return success result
    const result = {
      action: 'synced',
      tokenId: numericTokenId,
      traitsFound,
      traitsSynced: traitsFound, // Assume all traits were synced successfully
      errors: 0,
      durationMs: Date.now() - startTime
    };

    logger.debug(`üé® Trait sync completed for token ${numericTokenId} in ${result.durationMs}ms`);
    return result;

  } catch (error) {
    logger.error(`üé® Error syncing traits for ${ticker} token ${tokenId}:`, error);
    return { 
      action: 'error', 
      tokenId: parseInt(tokenId) || 0, 
      error: error.message,
      traitsFound: 0,
      traitsSynced: 0,
      errors: 1,
      durationMs: Date.now() - startTime
    };
  }
}

// Fast trait sync with one-way loading
export async function syncAllTraits(ticker, options = {}) {
  const {
    startTokenId = 1,
    endTokenId = 1000,
    batchSize = 50,
    delayBetweenTokens = 10,
    delayBetweenBatches = 100
  } = options;

  logger.info(`üé® Starting one-way trait sync for ${ticker} (tokens ${startTokenId}-${endTokenId})`);

  const results = {
    totalTokensProcessed: 0,
    totalTraitsFound: 0,
    totalTraitsSynced: 0,
    totalErrors: 0,
    tokensWithTraits: 0,
    tokensWithoutTraits: 0,
    tokensWithErrors: 0,
    tokensSkipped: 0,
    startTime: new Date().toISOString(),
    endTime: '',
    durationMs: 0,
    durationSeconds: ''
  };

  const startTime = Date.now();

  try {
    for (let tokenId = startTokenId; tokenId <= endTokenId; tokenId++) {
      try {
        const result = await syncSingleTokenTraits(ticker, tokenId);
        
        results.totalTokensProcessed++;
        
        if (result.action === 'synced') {
          results.totalTraitsFound += result.traitsFound;
          results.totalTraitsSynced += result.traitsSynced;
          results.totalErrors += result.errors;
          
          if (result.traitsFound > 0) {
            results.tokensWithTraits++;
          } else {
            results.tokensWithoutTraits++;
          }
        } else if (result.action === 'no_data') {
          results.tokensWithoutTraits++;
        } else if (result.action === 'error') {
          results.totalErrors++;
          results.tokensWithErrors++;
        }

        // Progress logging and batching
        if (tokenId % batchSize === 0) {
          const elapsed = (Date.now() - startTime) / 1000;
          const tokensPerSecond = results.totalTokensProcessed / elapsed;
          logger.info(`üé® Progress: ${tokenId}/${endTokenId} (${tokensPerSecond.toFixed(1)} tokens/sec, ${results.tokensWithTraits} with traits)`);

          await new Promise(resolve => setTimeout(resolve, delayBetweenBatches));
        } else {
          await new Promise(resolve => setTimeout(resolve, delayBetweenTokens));
        }

      } catch (error) {
        logger.error(`Failed to sync traits for token ${tokenId}:`, error);
        results.totalErrors++;
        results.tokensWithErrors++;
      }
    }

  } catch (error) {
    logger.error(`Error during one-way trait sync:`, error);
    results.totalErrors++;
  }

  // Calculate duration
  const endTime = Date.now();
  results.durationMs = endTime - startTime;
  results.durationSeconds = `${(results.durationMs / 1000).toFixed(2)}s`;
  results.endTime = new Date(endTime).toISOString();

  logger.info(`‚úÖ One-way trait sync completed for ${ticker}:`, results);
  
  return results;
}

export async function syncSalesHistoryForToken(ticker, tokenId) {
  try {
    // Ensure tokenId is a number
    const numericTokenId = parseInt(tokenId);
    if (isNaN(numericTokenId)) {
      throw new Error(`Invalid token ID: ${tokenId}`);
    }

    logger.info(`üîç [Token ${numericTokenId}] Starting smart sales history sync for ${ticker} token ${numericTokenId}`);

    // Step 1: Get current count from Kaspa API
    logger.debug(`üìä [Token ${numericTokenId}] Step 1: Getting sales count from Kaspa API...`);
    
    const kaspaCountResponse = await kaspaApi.fetchCompletedOrdersForToken(ticker, numericTokenId, {
      offset: 0,
      limit: 1, // We only need the count
      sortField: 'fullfillmentTimestamp',
      sortDirection: 'desc'
    });

    const kaspaCount = kaspaCountResponse.totalCount || 0;
    logger.debug(`üìä [Token ${numericTokenId}] Kaspa API reports ${kaspaCount} total sales`);

    // Step 2: Get existing sales IDs from our database (not just count!)
    logger.debug(`üóÑÔ∏è [Token ${numericTokenId}] Step 2: Getting existing sales IDs from database...`);
    
    const { data: existingSales, error: existingError } = await retrySupabaseCall(async () => {
      return await supabaseAdmin
        .from('sales_history')
        .select('id')
        .eq('token_id', numericTokenId);
    });

    if (existingError) {
      logger.error(`‚ùå [Token ${numericTokenId}] Error getting existing sales:`, existingError.message);
      return { 
        action: 'error', 
        tokenId: numericTokenId, 
        error: existingError.message,
        salesCount: 0,
        added: 0,
        updated: 0,
        skipped: 0,
        errors: 1
      };
    }

    const existingSalesIds = new Set((existingSales || []).map(sale => sale.id));
    const currentDbCount = existingSalesIds.size;
    
    logger.debug(`üóÑÔ∏è [Token ${numericTokenId}] Database has ${currentDbCount} sales records`);

    // Step 3: Compare counts - only proceed if Kaspa has more sales than we do
    if (kaspaCount <= currentDbCount) {
      logger.info(`‚úÖ [Token ${numericTokenId}] Sales history is up to date (Kaspa: ${kaspaCount}, DB: ${currentDbCount})`);
      return { 
        action: 'up_to_date', 
        tokenId: numericTokenId, 
        salesCount: kaspaCount,
        added: 0,
        updated: 0,
        skipped: currentDbCount,
        errors: 0
      };
    }

    const expectedNewSalesCount = kaspaCount - currentDbCount;
    logger.info(`üìà [Token ${numericTokenId}] Found ${expectedNewSalesCount} new sales to sync (Kaspa: ${kaspaCount}, DB: ${currentDbCount})`);

    // Step 4: Fetch ALL sales from Kaspa API and filter out existing ones
    logger.debug(`üì° [Token ${numericTokenId}] Step 4: Fetching all sales from Kaspa API to identify new ones...`);
    
    const allSalesResponse = await kaspaApi.fetchCompletedOrdersForToken(ticker, numericTokenId, {
      offset: 0,
      limit: kaspaCount, // Get all sales
      sortField: 'fullfillmentTimestamp',
      sortDirection: 'desc'
    });

    const allSales = allSalesResponse.orders || [];
    logger.debug(`üì° [Token ${numericTokenId}] Fetched ${allSales.length} total sales from API`);

    // Step 5: Filter out sales we already have
    const newSales = allSales.filter(sale => !existingSalesIds.has(sale.id));
    logger.debug(`üîç [Token ${numericTokenId}] Identified ${newSales.length} truly new sales to insert`);

    if (newSales.length === 0) {
      logger.info(`‚úÖ [Token ${numericTokenId}] No new sales to insert (all ${allSales.length} sales already exist)`);
      return { 
        action: 'up_to_date', 
        tokenId: numericTokenId, 
        salesCount: kaspaCount,
        added: 0,
        updated: 0,
        skipped: currentDbCount,
        errors: 0
      };
    }

    // Step 6: Insert only the truly new sales
    logger.debug(`üíæ [Token ${numericTokenId}] Step 6: Inserting ${newSales.length} new sales records...`);
    
    let addedCount = 0;
    let errorCount = 0;

    for (let i = 0; i < newSales.length; i++) {
      const sale = newSales[i];
      const saleIndex = i + 1;
      
      try {
        const salesRecord = {
          id: sale.id,
          token_id: parseInt(sale.tokenId),
          sale_price: sale.totalPrice,
          sale_date: new Date(sale.fullfillmentTimestamp).toISOString(),
          created_at: new Date().toISOString()
        };

        logger.debug(`üíæ [Token ${numericTokenId}] Inserting new sale ${saleIndex}/${newSales.length}: ID=${sale.id}`);

        await retrySupabaseCall(async () => {
          return await supabaseAdmin
            .from('sales_history')
            .insert(salesRecord);
        });
        
        addedCount++;
        logger.debug(`‚úÖ [Token ${numericTokenId}] Successfully inserted sale ${saleIndex}: ID=${sale.id}`);

      } catch (insertError) {
        // This should not happen since we filtered out existing IDs, but handle gracefully
        if (insertError.code === '23505') {
          logger.debug(`‚è≠Ô∏è [Token ${numericTokenId}] Sale ${saleIndex} already exists (race condition): ID=${sale.id}`);
          addedCount++; // Count as success since the record exists
        } else {
          logger.error(`‚ùå [Token ${numericTokenId}] Error inserting sale ${saleIndex}: ID=${sale.id}:`, insertError.message);
          errorCount++;
        }
      }

      // Small delay between inserts
      await new Promise(resolve => setTimeout(resolve, 25));
    }

    const result = {
      action: 'synced',
      tokenId: numericTokenId,
      salesCount: kaspaCount,
      added: addedCount,
      updated: 0,
      skipped: currentDbCount,
      errors: errorCount
    };

    logger.info(`‚úÖ [Token ${numericTokenId}] Smart sales history sync completed for ${ticker} token ${numericTokenId}:`, result);
    return result;

  } catch (error) {
    logger.error(`üí• [Token ${tokenId}] Error in smart sales history sync for ${ticker} token ${tokenId}:`, error);
    return { 
      action: 'error', 
      tokenId: parseInt(tokenId) || 0, 
      error: error.message,
      salesCount: 0,
      added: 0,
      updated: 0,
      skipped: 0,
      errors: 1
    };
  }
}

export async function syncAllSalesHistory(ticker, options = {}) {
  const {
    startTokenId = 1,
    endTokenId = 1000,
    batchSize = 5,
    delayBetweenTokens = 500,
    delayBetweenBatches = 3000
  } = options;

  logger.info(`üöÄ Starting smart sales history sync for ${ticker} (tokens ${startTokenId}-${endTokenId})`);

  const results = {
    totalTokensProcessed: 0,
    totalSalesFound: 0,
    totalSalesAdded: 0,
    totalSalesUpdated: 0,
    totalSalesSkipped: 0,
    totalErrors: 0,
    tokensWithSales: 0,
    tokensWithoutSales: 0,
    tokensUpToDate: 0,
    startTime: new Date().toISOString(),
    endTime: '',
    durationMs: 0,
    durationSeconds: ''
  };

  const startTime = Date.now();

  try {
    for (let tokenId = startTokenId; tokenId <= endTokenId; tokenId++) {
      try {
        const result = await syncSalesHistoryForToken(ticker, tokenId);
        
        results.totalTokensProcessed++;
        
        if (result.action === 'synced') {
          results.totalSalesFound += result.salesCount;
          results.totalSalesAdded += result.added;
          results.totalSalesUpdated += result.updated;
          results.totalSalesSkipped += result.skipped || 0;
          results.totalErrors += result.errors;
          
          if (result.salesCount > 0) {
            results.tokensWithSales++;
          } else {
            results.tokensWithoutSales++;
          }
        } else if (result.action === 'up_to_date') {
          results.totalSalesFound += result.salesCount;
          results.totalSalesSkipped += result.skipped || 0;
          results.tokensUpToDate++;
          
          if (result.salesCount > 0) {
            results.tokensWithSales++;
          } else {
            results.tokensWithoutSales++;
          }
        } else if (result.action === 'no_sales' || result.action === 'api_mismatch') {
          results.tokensWithoutSales++;
        } else if (result.action === 'error') {
          results.totalErrors++;
        }

        // Progress logging and batching
        if (tokenId % batchSize === 0) {
          logger.info(`üìä Sales sync progress: ${tokenId}/${endTokenId} tokens processed`, {
            tokensProcessed: results.totalTokensProcessed,
            salesFound: results.totalSalesFound,
            salesAdded: results.totalSalesAdded,
            tokensWithSales: results.tokensWithSales,
            tokensUpToDate: results.tokensUpToDate,
            errors: results.totalErrors
          });

          await new Promise(resolve => setTimeout(resolve, delayBetweenBatches));
        } else {
          await new Promise(resolve => setTimeout(resolve, delayBetweenTokens));
        }

      } catch (error) {
        logger.error(`Failed to sync sales history for token ${tokenId}:`, error);
        results.totalErrors++;
      }
    }

  } catch (error) {
    logger.error(`Error during smart sales history sync:`, error);
    results.totalErrors++;
  }

  // Calculate duration
  const endTime = Date.now();
  results.durationMs = endTime - startTime;
  results.durationSeconds = `${(results.durationMs / 1000).toFixed(2)}s`;
  results.endTime = new Date(endTime).toISOString();

  logger.info(`‚úÖ Smart sales history sync completed for ${ticker}:`, results);
  
  return results;
}

export async function syncSingleTokenListing(ticker, tokenId) {
  try {
    // Ensure tokenId is a number
    const numericTokenId = parseInt(tokenId);
    if (isNaN(numericTokenId)) {
      throw new Error(`Invalid token ID: ${tokenId}`);
    }

    // Fetch current listing from Kaspa API
    const kaspaOrderResponse = await kaspaApi.fetchListedOrdersForToken(ticker, numericTokenId);
    const kaspaOrder = kaspaOrderResponse.orders.length > 0 ? kaspaOrderResponse.orders[0] : null;

    // Check if token exists in our tokens table
    const tokenCheckResult = await retrySupabaseCall(async () => {
      return await supabaseAdmin
        .from('tokens')
        .select('token_id')
        .eq('token_id', numericTokenId)
        .maybeSingle();
    });

    const { data: existingToken, error: tokenCheckError } = tokenCheckResult;

    if (tokenCheckError) {
      logger.error(`Error checking existing token ${numericTokenId}:`, tokenCheckError);
    }

    // Only fetch token details if token doesn't exist
    if (!existingToken) {
      logger.debug(`Token ${numericTokenId} not found in database, fetching details from API`);
      
      const kaspaTokenDetails = await kaspaApi.fetchTokenTraits(ticker, numericTokenId);

      if (kaspaTokenDetails) {
        const { error: upsertTokenError } = await retrySupabaseCall(async () => {
          return await supabaseAdmin
            .from('tokens')
            .upsert({
              token_id: parseInt(kaspaTokenDetails.tokenId),
              rarity_rank: kaspaTokenDetails.rarityRank,
              is_legendary: kaspaTokenDetails.legendary,
            }, { onConflict: 'token_id' });
        });

        if (upsertTokenError) {
          logger.error(`Error upserting token ${numericTokenId} details:`, upsertTokenError);
        } else {
          logger.debug(`Synced token ${numericTokenId} details to 'tokens' table.`);
        }

        await syncTokenTraits(ticker, numericTokenId, kaspaTokenDetails.traits);
      } else {
        logger.warn(`No token details found for ${ticker} token ${numericTokenId}. Skipping trait sync.`);
      }
    } else {
      logger.debug(`Token ${numericTokenId} already exists in database, skipping trait fetching`);
    }

    // Get current active listing from database
    const { data: currentListing, error: fetchError } = await retrySupabaseCall(async () => {
      return await supabaseAdmin
        .from('listings')
        .select('*')
        .eq('ticker', ticker)
        .eq('token_id', numericTokenId)
        .eq('status', 'active')
        .maybeSingle();
    });

    if (fetchError) {
      throw fetchError;
    }

    let result = { action: 'no_change', tokenId: numericTokenId };

    if (kaspaOrder && !currentListing) {
      // New listing found
      const newListing = {
        kaspa_order_id: kaspaOrder.id,
        ticker: kaspaOrder.ticker,
        token_id: parseInt(kaspaOrder.tokenId),
        total_price: kaspaOrder.totalPrice,
        seller_wallet_address: kaspaOrder.sellerWalletAddress,
        rarity_rank: kaspaOrder.rarityRank,
        required_kaspa: kaspaOrder.requiredKaspa,
        kaspa_created_at: kaspaOrder.createdAt,
        source: 'kaspa_api',
        is_owner: kaspaOrder.isOwner || false,
        status: 'active'
      };

      const { error: insertError } = await retrySupabaseCall(async () => {
        return await supabaseAdmin
          .from('listings')
          .insert(newListing);
      });

      if (insertError) {
        throw insertError;
      }

      result = { action: 'added', tokenId: numericTokenId, orderId: kaspaOrder.id };
      logger.debug(`Added new listing for ${ticker} token ${numericTokenId}`);

    } else if (!kaspaOrder && currentListing) {
      // Listing no longer exists
      const systemUserId = await getSystemUserId();
      
      const updateData = {
        status: 'api_sync_removed',
        deactivated_at: new Date().toISOString(),
        deactivated_by: systemUserId,
        updated_at: new Date().toISOString()
      };

      const { error: updateError } = await retrySupabaseCall(async () => {
        return await supabaseAdmin
          .from('listings')
          .update(updateData)
          .eq('id', currentListing.id);
      });

      if (updateError) {
        logger.warn(`Could not mark listing ${currentListing.id} as removed:`, updateError.message);
        result = { action: 'error', tokenId: numericTokenId, error: updateError.message };
      } else {
        result = { action: 'removed', tokenId: numericTokenId, listingId: currentListing.id };
        logger.debug(`Removed listing for ${ticker} token ${numericTokenId}`);
      }

    } else if (kaspaOrder && currentListing) {
      // Check if listing needs updating
      const needsUpdate = 
        currentListing.kaspa_order_id !== kaspaOrder.id ||
        parseFloat(currentListing.total_price) !== kaspaOrder.totalPrice ||
        currentListing.seller_wallet_address !== kaspaOrder.sellerWalletAddress ||
        currentListing.rarity_rank !== kaspaOrder.rarityRank ||
        parseFloat(currentListing.required_kaspa || 0) !== (kaspaOrder.requiredKaspa || 0);

      if (needsUpdate) {
        const updateData = {
          kaspa_order_id: kaspaOrder.id,
          total_price: kaspaOrder.totalPrice,
          seller_wallet_address: kaspaOrder.sellerWalletAddress,
          rarity_rank: kaspaOrder.rarityRank,
          required_kaspa: kaspaOrder.requiredKaspa,
          kaspa_created_at: kaspaOrder.createdAt,
          is_owner: kaspaOrder.isOwner || false,
          updated_at: new Date().toISOString()
        };

        const { error: updateError } = await retrySupabaseCall(async () => {
          return await supabaseAdmin
            .from('listings')
            .update(updateData)
            .eq('id', currentListing.id);
        });

        if (updateError) {
          logger.warn(`Could not update listing ${currentListing.id}:`, updateError.message);
          result = { action: 'error', tokenId: numericTokenId, error: updateError.message };
        } else {
          result = { action: 'updated', tokenId: numericTokenId, orderId: kaspaOrder.id };
          logger.debug(`Updated listing for ${ticker} token ${numericTokenId}`);
        }
      }
    }

    return result;

  } catch (error) {
    logger.error(`Error syncing token ${tokenId} for ${ticker}:`, error);
    return { action: 'error', tokenId: parseInt(tokenId) || 0, error: error.message };
  }
}

// Helper function to check if listing details have changed
function hasListingChanged(dbListing, kaspaOrder) {
  return (
    parseFloat(dbListing.total_price) !== kaspaOrder.totalPrice ||
    dbListing.seller_wallet_address !== kaspaOrder.sellerWalletAddress ||
    dbListing.rarity_rank !== kaspaOrder.rarityRank ||
    parseFloat(dbListing.required_kaspa || 0) !== (kaspaOrder.requiredKaspa || 0) ||
    dbListing.is_owner !== (kaspaOrder.isOwner || false)
  );
}

export async function syncAllTickerListings(ticker) {
  logger.info(`Starting conservative sync for ticker: ${ticker}`);
  
  try {
    // Check network connectivity first
    logger.info(`Checking network connectivity to Supabase...`);
    const connectivity = await checkSupabaseConnectivity();
    
    if (!connectivity.connected) {
      throw new Error(`Network connectivity check failed: ${connectivity.error}`);
    }
    
    logger.info(`Network connectivity confirmed (latency: ${connectivity.latency}ms)`);

    // Test API connectivity
    logger.info(`Testing API connectivity for ${ticker}...`);
    
    try {
      const testResponse = await kaspaApi.fetchListedOrders(ticker, {
        offset: 0,
        limit: 1
      });
      logger.info(`API connectivity test successful. Found ${testResponse.totalCount} total orders for ${ticker}`);
    } catch (testError) {
      logger.error(`API connectivity test failed for ${ticker}:`, testError.message);
      throw new Error(`API connectivity test failed: ${testError.message}`);
    }

    // Fetch all current listings from Kaspa API in smaller batches
    const allOrders = [];
    let offset = 0;
    const batchSize = 25;
    let hasMore = true;
    let batchCount = 0;

    logger.info(`Fetching all orders from Kaspa API for ${ticker}...`);

    while (hasMore) {
      batchCount++;
      logger.debug(`Fetching batch ${batchCount} (offset: ${offset}, limit: ${batchSize})`);

      const response = await kaspaApi.fetchListedOrders(ticker, {
        offset,
        limit: batchSize
      });

      allOrders.push(...response.orders);
      
      logger.debug(`Batch ${batchCount} completed: received ${response.orders.length} orders`, {
        batchNumber: batchCount,
        ordersInBatch: response.orders.length,
        totalOrdersSoFar: allOrders.length,
        apiTotalCount: response.totalCount
      });

      hasMore = response.orders.length === batchSize;
      offset += batchSize;

      if (hasMore) {
        logger.debug(`Waiting 500ms before next batch...`);
        await new Promise(resolve => setTimeout(resolve, 500));
      }
    }

    logger.info(`Kaspa API fetch completed: ${allOrders.length} total orders received for ${ticker}`, {
      ticker,
      totalOrdersFromAPI: allOrders.length,
      batchesFetched: batchCount
    });

    // Get ALL active listings from database for this ticker
    logger.debug(`Fetching active listings from database for ${ticker}...`);
    
    const { data: activeDbListings, error: fetchError } = await retrySupabaseCall(async () => {
      return await supabaseAdmin
        .from('listings')
        .select('*')
        .eq('ticker', ticker)
        .eq('status', 'active');
    });

    if (fetchError) {
      throw fetchError;
    }

    const currentActiveListings = activeDbListings || [];

    logger.info(`Database query completed: ${currentActiveListings.length} active listings found in database for ${ticker}`, {
      ticker,
      activeListingsInDB: currentActiveListings.length
    });

    // Create maps for efficient lookups
    const currentActiveOrderIds = new Set(
      currentActiveListings.map(listing => listing.kaspa_order_id).filter(Boolean)
    );
    const newOrderIds = new Set(allOrders.map(order => order.id));
    
    const activeDbListingsMap = new Map();
    currentActiveListings.forEach(listing => {
      if (listing.kaspa_order_id) {
        activeDbListingsMap.set(listing.kaspa_order_id, listing);
      }
    });

    // Find orders to add
    const ordersToAdd = allOrders.filter(order => !currentActiveOrderIds.has(order.id));
    
    // Find orders to remove
    const listingsToRemove = currentActiveListings.filter(
      listing => !newOrderIds.has(listing.kaspa_order_id)
    );

    // Find orders that need updates
    const listingsToUpdate = [];
    
    for (const order of allOrders) {
      if (activeDbListingsMap.has(order.id)) {
        const dbListing = activeDbListingsMap.get(order.id);
        
        if (hasListingChanged(dbListing, order)) {
          listingsToUpdate.push({
            kaspaOrder: order,
            dbListing: dbListing
          });
        }
      }
    }

    logger.info(`Conservative sync analysis completed for ${ticker}:`, {
      ticker,
      totalOrdersFromAPI: allOrders.length,
      activeListingsInDB: currentActiveListings.length,
      ordersToAdd: ordersToAdd.length,
      listingsToUpdate: listingsToUpdate.length,
      listingsToRemove: listingsToRemove.length,
      expectedFinalActiveCount: currentActiveListings.length + ordersToAdd.length - listingsToRemove.length
    });

    // Get system user ID for automated operations
    const systemUserId = await getSystemUserId();

    let addedCount = 0;
    let updatedCount = 0;
    let removedCount = 0;
    let errorCount = 0;

    // Add new listings with conservative batch processing
    if (ordersToAdd.length > 0) {
      logger.info(`Adding ${ordersToAdd.length} new listings for ${ticker}...`);
      
      const addBatchSize = 5;
      for (let i = 0; i < ordersToAdd.length; i += addBatchSize) {
        const batch = ordersToAdd.slice(i, i + addBatchSize);
        const listingsToInsert = batch.map(order => ({
          kaspa_order_id: order.id,
          ticker: order.ticker,
          token_id: parseInt(order.tokenId),
          total_price: order.totalPrice,
          seller_wallet_address: order.sellerWalletAddress,
          rarity_rank: order.rarityRank,
          required_kaspa: order.requiredKaspa,
          kaspa_created_at: order.createdAt,
          source: 'kaspa_api',
          is_owner: order.isOwner || false,
          status: 'active'
        }));

        try {
          await retrySupabaseCall(async () => {
            return await supabaseAdmin
              .from('listings')
              .insert(listingsToInsert);
          });
          
          addedCount += batch.length;
          logger.debug(`Added batch of ${batch.length} listings (${addedCount}/${ordersToAdd.length})`);
        } catch (insertError) {
          logger.error(`Error inserting batch of listings:`, insertError);
          errorCount += batch.length;
        }

        if (i + addBatchSize < ordersToAdd.length) {
          await new Promise(resolve => setTimeout(resolve, 1000));
        }
      }
      
      logger.info(`Successfully added ${addedCount} new listings for ${ticker}`);
    } else {
      logger.info(`No new listings to add for ${ticker}`);
    }

    // Handle removals with conservative approach
    if (listingsToRemove.length > 0) {
      logger.info(`Marking ${listingsToRemove.length} removed listings for ${ticker}...`);
      
      for (const listing of listingsToRemove) {
        try {
          const updateData = {
            status: 'api_sync_removed',
            deactivated_at: new Date().toISOString(),
            deactivated_by: systemUserId,
            updated_at: new Date().toISOString()
          };

          await retrySupabaseCall(async () => {
            return await supabaseAdmin
              .from('listings')
              .update(updateData)
              .eq('id', listing.id);
          });

          removedCount++;
          logger.debug(`Successfully marked listing ${listing.id} as api_sync_removed`);

        } catch (removalError) {
          logger.error(`Failed to mark listing ${listing.id} as removed:`, removalError.message);
          errorCount++;
        }

        await new Promise(resolve => setTimeout(resolve, 200));
      }

      logger.info(`Removal processing completed for ${ticker}: ${removedCount} marked as removed`);
    } else {
      logger.info(`No listings to mark as removed for ${ticker}`);
    }

    // Final verification
    const { data: finalListings, error: finalCountError } = await retrySupabaseCall(async () => {
      return await supabaseAdmin
        .from('listings')
        .select('id', { count: 'exact' })
        .eq('ticker', ticker)
        .eq('status', 'active');
    });

    if (finalCountError) {
      logger.error(`Error getting final count for ${ticker}:`, finalCountError);
    } else {
      const finalCount = finalListings?.length || 0;
      logger.info(`Conservative sync completed for ${ticker}:`, {
        ticker,
        kaspaAPITotal: allOrders.length,
        finalDatabaseCount: finalCount,
        added: addedCount,
        updated: updatedCount,
        removed: removedCount,
        errors: errorCount,
        discrepancy: allOrders.length - finalCount,
        syncSuccessful: allOrders.length === finalCount
      });

      if (allOrders.length !== finalCount) {
        logger.warn(`DISCREPANCY DETECTED for ${ticker}: Expected ${allOrders.length} but have ${finalCount} in database`);
        logger.info(`This may be due to errors during sync. Check error count: ${errorCount}`);
      }
    }

    return {
      added: addedCount,
      updated: updatedCount,
      removed: removedCount,
      errors: errorCount,
      total: allOrders.length,
      finalDatabaseCount: finalListings?.length || 0
    };

  } catch (error) {
    logger.error(`Error syncing all listings for ${ticker}:`, error);
    throw error;
  }
}

// New function to sync Kaspunk owners
export async function syncKaspunkOwners() {
  const startTime = Date.now();
  logger.info('üßë‚Äçü§ù‚Äçüßë Starting Kaspunk owners sync...');

  try {
    // Step 1: Fetch owners data from Kaspa API
    const { holders, totalHolders, totalMinted, totalSupply } = await kaspaApi.fetchKaspunkOwners();
    
    if (!holders || holders.length === 0) {
      logger.warn('No holders data returned from API');
      return {
        action: 'error',
        error: 'No holders data returned from API',
        totalHolders: 0,
        processedHolders: 0,
        addedHolders: 0,
        updatedHolders: 0,
        errors: 1,
        durationMs: Date.now() - startTime
      };
    }

    logger.info(`Fetched ${holders.length} holders from API (API reports ${totalHolders} total holders)`);

    // Step 2: Update collection stats first
    try {
      const statsData = {
        total_supply: totalSupply || 1000,
        total_minted: totalMinted || 1000,
        total_holders: totalHolders || holders.length,
        average_holding: totalHolders ? (totalMinted / totalHolders) : 0,
        last_synced_at: new Date().toISOString(),
        updated_at: new Date().toISOString()
      };

      // Check if stats record exists
      const { data: existingStats } = await retrySupabaseCall(async () => {
        return await supabaseAdmin
          .from('kaspunk_collection_stats')
          .select('id')
          .limit(1);
      });

      if (existingStats && existingStats.length > 0) {
        // Update existing stats
        await retrySupabaseCall(async () => {
          return await supabaseAdmin
            .from('kaspunk_collection_stats')
            .update(statsData)
            .eq('id', existingStats[0].id);
        });
        logger.info('Updated collection stats');
      } else {
        // Insert new stats
        await retrySupabaseCall(async () => {
          return await supabaseAdmin
            .from('kaspunk_collection_stats')
            .insert(statsData);
        });
        logger.info('Inserted new collection stats');
      }
    } catch (statsError) {
      logger.error('Error updating collection stats:', statsError);
      // Continue with owner sync even if stats update fails
    }

    // Step 3: Process each holder and upsert to database
    let addedCount = 0;
    let updatedCount = 0;
    let errorCount = 0;
    const batchSize = 50;

    logger.info(`Processing ${holders.length} holders in batches of ${batchSize}...`);

    for (let i = 0; i < holders.length; i += batchSize) {
      const batch = holders.slice(i, i + batchSize);
      
      // CRITICAL FIX: Deduplicate wallet addresses within each batch
      const uniqueHoldersMap = new Map();
      batch.forEach(holder => {
        const walletAddress = holder.owner;
        if (!uniqueHoldersMap.has(walletAddress)) {
          uniqueHoldersMap.set(walletAddress, holder);
        } else {
          // If duplicate found, keep the one with higher token count
          const existing = uniqueHoldersMap.get(walletAddress);
          if (holder.count > existing.count) {
            uniqueHoldersMap.set(walletAddress, holder);
          }
        }
      });
      
      const deduplicatedBatch = Array.from(uniqueHoldersMap.values());
      
      if (deduplicatedBatch.length !== batch.length) {
        logger.debug(`Deduplicated batch ${Math.floor(i/batchSize) + 1}: ${batch.length} -> ${deduplicatedBatch.length} unique holders`);
      }
      
      // Create holder records for the batch
      const holderRecords = deduplicatedBatch.map(holder => ({
        wallet_address: holder.owner,
        token_count: holder.count,
        updated_at: new Date().toISOString()
      }));

      try {
        // Use upsert with onConflict to handle duplicates properly
        const { error } = await retrySupabaseCall(async () => {
          return await supabaseAdmin
            .from('kaspunk_owners')
            .upsert(holderRecords, {
              onConflict: 'wallet_address',
              returning: false // Don't need to return data for performance
            });
        });

        if (error) {
          logger.error(`Error upserting batch of holders (${i+1}-${i+deduplicatedBatch.length}):`, error);
          errorCount += deduplicatedBatch.length;
        } else {
          // Since we're using upsert without returning data, we can't accurately track added vs updated
          // So we'll just count them all as processed
          const processedCount = deduplicatedBatch.length;
          
          // For reporting purposes, estimate added vs updated (not accurate but gives a sense)
          addedCount += Math.floor(processedCount * 0.1); // Assume ~10% are new
          updatedCount += processedCount - Math.floor(processedCount * 0.1); // The rest are updates
          
          logger.debug(`Processed batch of ${deduplicatedBatch.length} holders (${i+1}-${Math.min(i+batchSize, holders.length)}/${holders.length})`);
        }

        // Add a small delay between batches
        if (i + batchSize < holders.length) {
          await new Promise(resolve => setTimeout(resolve, 500));
        }
      } catch (batchError) {
        logger.error(`Error processing batch of holders (${i+1}-${i+deduplicatedBatch.length}):`, batchError);
        errorCount += deduplicatedBatch.length;
      }
    }

    // Calculate duration and prepare result
    const endTime = Date.now();
    const durationMs = endTime - startTime;
    const durationSeconds = (durationMs / 1000).toFixed(2);

    const result = {
      action: 'synced',
      totalHolders: totalHolders || holders.length,
      processedHolders: holders.length,
      addedHolders: addedCount,
      updatedHolders: updatedCount,
      errors: errorCount,
      durationMs,
      durationSeconds: `${durationSeconds}s`,
      startTime: new Date(startTime).toISOString(),
      endTime: new Date(endTime).toISOString()
    };

    logger.info(`‚úÖ Kaspunk owners sync completed:`, result);
    return result;

  } catch (error) {
    logger.error('Error syncing Kaspunk owners:', error);
    
    return {
      action: 'error',
      error: error.message,
      totalHolders: 0,
      processedHolders: 0,
      addedHolders: 0,
      updatedHolders: 0,
      errors: 1,
      durationMs: Date.now() - startTime,
      durationSeconds: `${((Date.now() - startTime) / 1000).toFixed(2)}s`,
      startTime: new Date(startTime).toISOString(),
      endTime: new Date().toISOString()
    };
  }
}